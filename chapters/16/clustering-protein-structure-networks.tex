\graphicspath{{chapters/16/images}}
\chapter{Clustering and protein structure networks}

\section{Clustering}
The aim of clustering is to find a way to group a set of data into clusters of similar properties.
This is done because:

\begin{multicols}{2}
	\begin{itemize}
		\item Labelling is expensive.
		\item To gain insight into the structure of data.
		\item Find prototypes in the data.
	\end{itemize}
\end{multicols}

In molecular simulations data usually refers to protein, DNA or RNA conformations.
So, given a set of data points, each described by a set of attributes, the clusters have to be found such that:

\begin{multicols}{2}
	\begin{itemize}
		\item Intra-cluster similarity is maximized: all the points in a cluster are as much similar as possible.
		\item Inter-cluster similarity is minimized: all the points between clusters are as much dissimilar between each other.
	\end{itemize}
\end{multicols}

	\subsection{Distance measures}
	To define similarity let $O_1$ and $O_2$ be two objects from the universe of possible objects.
	The distance or dissimilarly between $O_1$ and $O_2$ is a real number $D(O_1, O_2)$.
	A distance measure should have the following properties:

	\begin{itemize}
		\item Symmetry: $D(A,B) = D(B, A)$.
		\item Constancy of self-similarity: $D(A, A) = 0$.
		\item Positivity (separation): $(A, B) = 0\Leftrightarrow A=B$.
		\item Triangular inequality: $D(A, B) \le D(A, C) + D(B, C)$.
	\end{itemize}

	An example of this for molecular dynamics and two protein conformation is the root mean squared deviation, computing the all-to-all RMSD matrix.

	\subsection{Types of clustering}

	\begin{itemize}
		\item Hierarchical algorithms: create a hierarchical decomposition of the set of objects using some criterion.
		\item Partitional algorithms: construct various partitions and then evaluate them by some criterion,
	\end{itemize}

	\subsection{Dendrograms}
	In a dendrogram (\ref{fig:dendrogram}) the similarity between two objects is represented as the height of the lowest internal node they share.
	Each object is represented as a leaf.
	Dendrograms give a direct visual representation of the groups of data.

	\begin{figure}[H]
		\includegraphics[width=\textwidth]{dendrogram}
		\caption{Dendrogram structure}
		\label{fig:dendrogram}
	\end{figure}

		\subsection{Interpretation}
		Hierarchical clustering sometimes show pattern that are meaningless or spurious.
		So the interpretation of the algorithm has to be performed.

\section{Correct number of clusters}

	\begin{figure}[H]
		\includegraphics[width=\textwidth]{correct-number}
		\caption{Correct number of clusters}
		\label{fig:correct-number}
	\end{figure}

	\subsection{Outliers}

	\begin{figure}[H]
		\includegraphics[width=\textwidth]{outliers}
		\caption{Outliers}
		\label{fig:dendrogram}
	\end{figure}

\section{Hierarchical clustering}
The number of dendrograms $D$ with $n$ leafs are:

$$D = \frac{2n-3)!}{2^{n-2}(n-2)!}$$

They can be built with two approaches:

\begin{itemize}
	\item Bottom-up or agglomerative approach: each item is grouped alone into its own cluster and then the best pair to merge into a new cluster is found.
		This is repeated until all clusters are fused together.
	\item Top-down or divisive approach: all the data is grouped into a single cluster, then the best division into two cluster is chosen and this is repeated recursively on both sides.
\end{itemize}

\section{Distance measures}
The distance between objects in a cluster or clusters can be computed in several ways:

\begin{itemize}
	\item Single linkage or nearest neighbour: the distance between two clusters is determined by the distance of the two closest objects (nearest neighbours) in the different clusters.
	\item Complete linkage or furthest neighbour: the distance between two clusters is the greatest distance between two objects in the different clusters.
	\item Group average linkage: the distance between two clusters is computed as the average distance between all pairs of objects in the two different clusters.
\end{itemize}

\section{Hierarchical methods}

\begin{itemize}
	\item No need to specify the number of clusters in advance.
	\item The hierarchical nature maps nicely onto human intuition for some domains.
	\item They do not scale well: $O(n^2)$.
	\item The interpretation of the results is very subjective.
\end{itemize}

\section{Partitional clustering}
In partitional clustering each item is placed in exactly one of $K$ non-overlapping clusters.
The number of cluster is provided as input.

	\subsection{K-means}

	\begin{enumerate}
		\item Choose the value $k$.
		\item Initialize the $k$ cluster centres randomly.
		\item Decide the class memberships of the $N$ objects by assigning them to the nearest cluster centre.
		\item Re-estimate the $k$-cluster centres by assuming the memberships found are correct.
		\item If non of the $N$ objects changed memberships in the last iteration exit, otherwise go back to step $3$.
	\end{enumerate}

		\subsubsection{Conclusion}

		\begin{itemize}
			\item Relatively efficient: $O(tkn)$ where $n$ is the number of objects, $k$ is the number of clusters and $t$ the number of iterations.
			\item It often terminates at a local optimum.
			\item It is applicable only when a mean can be defined.
			\item The number of cluster has to be specified in advance.
			\item It is unable to handle noisy data or outliers.
			\item It is not suitable to discover clusters with non-convex shapes.
		\end{itemize}

	\subsection{Sum of squared errors}

	$$SE_{K_i} = \sum\limits_{j=1}^m[D(C_{ij}, C_{K_i})]^2$$

	$$SE_K = \sum\limits_{i=1}^jSE_{K_i}$$

	The objective function.

	\subsection{Choosing K}
	The optimal $k$ is the one such that the $\min SE$ is found.
	It can be chosen running the algorithm with iteratively increasing value of $k$ and looking at the Knee or elbow plot of $k$ in relation with $SE$ as in \ref{fig:elbow}.

	\begin{figure}[H]
		\includegraphics[width=\textwidth]{elbow}
		\caption{Knee or elbow plot}
		\label{fig:elbow}
	\end{figure}

\section{Protein structure networks}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{psn}
	\caption{Protein structure network}
	\label{fig:psn}
\end{figure}

	\subsection{PyInteraph}
	In PyInteraph the nodes are the side chains of protein residues.
	The edges can be defined by:

	\begin{multicols}{2}
		\begin{itemize}
			\item Distance.
			\item Atomic contacts.
			\item Van der Waals interactions.
			\item Interaction energy.
		\end{itemize}
	\end{multicols}

	Graph analysis approach to the intramolecular interaction network IIN.

	\subsection{Classes of interactions}

		\subsubsection{Hydrophobic contacts}
		In the hydrophobic contacts the centre of mass of the two side chains are distant less than $5\si{\angstrom}$.
		The centres of mass is specified by the force field for different masses.
		It has default residues:

		\begin{multicols}{4}
			\begin{itemize}
				\item Ala.
				\item Ile.
				\item Val.
				\item Phe.
				\item Met.
				\item Trp.
				\item Pro.
			\end{itemize}
		\end{multicols}

		\subsubsection{Salt bridges}
		Salt bridges are formed between atom pairs belonging to two charged groups of two different residues with distance less than $4.5\si{\angstrom}$.

		\subsubsection{Hydrogen bonds}
		Hydrogen bond happen when the distance between the acceptor and the hydrogen atom is less than $3.5\si{\angstrom}$ and the donor-hydrogen-acceptor angle is greater than $120^\circ$.

	\subsection{Persistence}
	Persitence is the fraction of the number of structures in the ensemble in which the interaction was observed.
	The Edge weight is the persistence value.
	For hydrogen-bond one or more interactions may exist between two residues.

		\subsection{Persistence threshold}
		A connected component is a subgraph in which a path exists between any two vertices, but no path exist to any other vertices of the main graph: there are no edges connecting two connected components.

	\subsection{Graph analysis}

	\begin{itemize}
		\item Highly connected residues or hubs with more than $3$ or $4$ edges allow the creation for a list of hubs and the connectivity degree for each of them.
		\item Connected components.
		\item Shortest path between two specified residues.
	\end{itemize};

	\subsection{Examples}

		\subsubsection{p53 DNA binding domain}
		Hydrophobic interactions play a crucial role in the stabilization of the protein core and in the maintenance of the 3D structure and stability.

		\begin{figure}[H]
			\includegraphics[width=\textwidth]{p53}
			\caption{p53}
			\label{fig:p53}
		\end{figure}

		\subsubsection{Vibrio proteinase (VAP)}
		Salt bridges or hydrogen bonds are highly flexible and cooperatively organized in networks across the protein structure for temperature adapted organisms.

		\begin{figure}[H]
			\includegraphics[width=\textwidth]{vap}
			\caption{VAP}
			\label{fig:vap}
		\end{figure}
